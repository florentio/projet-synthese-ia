# ğŸš€ Complete MLOps CI/CD Pipeline for Telco Churn Prediction

## ğŸ“ Project Directory Structure

```
telco-churn-mlops/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ml-pipeline.yml              # Main CI/CD pipeline
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ fastapi_app/
â”‚   â”‚   â”œâ”€â”€ main.py                      # FastAPI application
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ flask-app/
â”‚   â”‚   â”œâ”€â”€ app.py                       # Flask application
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ streamlit_app/
â”‚   â”‚   â”œâ”€â”€ dashboard.py                 # Streamlit dashboard
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ monitoring_service.py        # Continuous monitoring
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ templates/                           # Flask HTML templates
â”‚   â”œâ”€â”€ base.html
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ predict.html
â”‚   â””â”€â”€ model_info.html
â”œâ”€â”€ scripts/                            # Utility and automation scripts
â”‚   â”œâ”€â”€ data_preparation.py
â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”œâ”€â”€ drift_monitoring.py
â”‚   â”œâ”€â”€ model_evaluation.py
â”‚   â”œâ”€â”€ generate_monitoring_reports.py
â”‚   â”œâ”€â”€ integration_tests.py
â”‚   â”œâ”€â”€ performance_test.py
â”‚   â”œâ”€â”€ prediction_smoke_test.py
â”‚   â””â”€â”€ setup_alerts.py
â”œâ”€â”€ docker/                             # Docker configurations
â”‚   â”œâ”€â”€ Dockerfile.flask-app
â”‚   â”œâ”€â”€ Dockerfile.fastapi-app
â”‚   â”œâ”€â”€ Dockerfile.streamlit-app
â”‚   â”œâ”€â”€ Dockerfile.mlflow
â”‚   â”œâ”€â”€ Dockerfile.training
â”‚   â”œâ”€â”€ Dockerfile.dvc
â”‚   â””â”€â”€ Dockerfile.monitoring
â”œâ”€â”€ monitoring/                         # Monitoring configurations
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â”œâ”€â”€ alert_rules.json
â”‚   â””â”€â”€ grafana/
â”‚       â”œâ”€â”€ dashboards/
â”‚       â””â”€â”€ provisioning/
â”œâ”€â”€ nginx/                              # Reverse proxy configurations
â”‚   â”œâ”€â”€ nginx.conf
â”‚   â”œâ”€â”€ nginx.prod.conf
â”‚   â””â”€â”€ ssl/
â”œâ”€â”€ data/                               # Data directories (DVC tracked)
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ telco_churn_raw.csv
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ telco_churn.csv
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ training_features.csv
â”‚   â”œâ”€â”€ reference_data.csv
â”‚   â””â”€â”€ monitoring/
â”‚       â””â”€â”€ current_data.csv
â”œâ”€â”€ model/                              # Model artifacts (DVC tracked)
â”‚   â”œâ”€â”€ best_churn_model.joblib
â”‚   â”œâ”€â”€ model_metadata.json
â”‚   â””â”€â”€ evaluation_metrics.png
â”œâ”€â”€ reports/                            # Generated reports
â”‚   â”œâ”€â”€ drift_reports/
â”‚   â”œâ”€â”€ evaluation_reports/
â”‚   â””â”€â”€ monitoring_dashboards/
â”œâ”€â”€ train.py                            # Original training script
â”œâ”€â”€ train_with_mlflow.py                # MLflow enhanced training
â”œâ”€â”€ dvc.yaml                            # DVC pipeline configuration
â”œâ”€â”€ params.yaml                         # DVC parameters
â”œâ”€â”€ .dvcignore                          # DVC ignore file
â”œâ”€â”€ docker-compose.yml                  # Local development
â”œâ”€â”€ requirements.txt                    # Core dependencies
â”œâ”€â”€ requirements-dev.txt                # Development dependencies
â”œâ”€â”€ requirements-fastapi.txt            # FastAPI specific
â”œâ”€â”€ requirements-streamlit.txt          # Streamlit specific
â”œâ”€â”€ requirements-prod.txt               # Production dependencies
â”œâ”€â”€ requirements-monitoring.txt         # Monitoring dependencies
â”œâ”€â”€ README.md                           # Project documentation
â””â”€â”€ .gitignore                          # Git ignore file
```

## ğŸ”„ CI/CD Pipeline Flow

### 1. **Trigger Events**
- Push to `main` or `develop` branches
- Pull request creation
- Manual workflow dispatch
- Scheduled runs (daily/weekly)

### 2. **Pipeline Stages**

#### **Stage 1: Code Quality & Testing**
- **Black** formatting checks
- **isort** import organization
- **flake8** linting and code style
- **mypy** type checking
- **bandit** security scanning
- **pytest** unit and integration tests
- **Coverage** reporting

#### **Stage 2: Data Validation (DVC + Evidently AI)**
- Pull latest data with **DVC**
- Data quality validation
- **Evidently AI** drift detection
- Schema validation
- Generate retrain flag if drift detected

#### **Stage 3: Model Training (Conditional)**
- Triggered if data drift detected or manual override
- **MLflow** experiment tracking
- Multi-model training and comparison
- Hyperparameter tuning with **GridSearchCV**
- Model registration in **MLflow Model Registry**
- **DVC** artifact versioning

#### **Stage 4: Docker Image Building**
- Multi-service image builds (Flask, FastAPI, Streamlit)
- **GitHub Container Registry** push
- Layer caching optimization
- Security scanning

#### **Stage 5: Monitoring Setup**
- **Evidently AI** monitoring reports generation
- Performance baseline establishment
- Alert rule configuration

#### **Stage 6: Deployment**
- Staging environment deployment
- Integration testing
- Production deployment (if tests pass)
- **Kubernetes** or **Docker Compose** orchestration

#### **Stage 7: Post-Deployment Validation**
- Health checks for all services
- Prediction smoke tests
- Performance baseline validation

## ğŸ”§ Tool Integration Details

### **GitHub Actions**
- **Workflow orchestration** and automation
- **Secrets management** for credentials
- **Matrix builds** for multiple services
- **Conditional execution** based on data drift
- **Artifact storage** for reports and logs

### **Docker**
- **Multi-stage builds** for optimization
- **Service isolation** with dedicated containers
- **Health checks** for reliability
- **Resource limits** for stability
- **Security scanning** with best practices

### **MLflow**
- **Experiment tracking** with metrics, parameters, artifacts
- **Model registry** for version management
- **Model comparison** and promotion workflows
- **Artifact storage** (S3/GCS integration)
- **REST API** for programmatic access

### **DVC (Data Version Control)**
- **Data pipeline** definition and execution
- **Reproducible workflows** with parameters
- **Data and model versioning**
- **Remote storage** integration (S3/GCS)
- **Pipeline visualization** and DAG management

### **Evidently AI**
- **Data drift detection** with statistical tests
- **Model performance monitoring**
- **Data quality reports** with detailed analysis
- **Interactive HTML reports**
- **Integration with MLflow** for tracking

### **FastAPI**
- **High-performance inference** with async processing
- **Batch prediction** capabilities
- **OpenAPI documentation** (Swagger/ReDoc)
- **Request validation** with Pydantic
- **Monitoring endpoints** for health and metrics

### **Streamlit**
- **Interactive dashboards** for business users
- **Real-time monitoring** visualization
- **Model explainability** with SHAP integration
- **Batch analysis** interface
- **Performance tracking** charts

## âš™ï¸ Setup Instructions

### **1. Repository Setup**

```bash
# Clone and setup repository
git clone https://github.com/your-org/telco-churn-mlops.git
cd telco-churn-mlops

# Initialize DVC
dvc init
dvc remote add -d myremote s3://your-dvc-bucket/data
```

### **2. Environment Configuration**

```bash
# Create development environment
python -m venv mlops_env
source mlops_env/bin/activate  # Windows: mlops_env\Scripts\activate

# Install dependencies
pip install -r requirements-dev.txt
```

### **3. Data Setup**

```bash
# Add your data to DVC tracking
dvc add data/raw/telco_churn_raw.csv

# For Google Cloud Storage
dvc remote add -d myremote gs://your-bucket-name/data

dvc push

# Create reference dataset for monitoring
python scripts/data_preparation.py
```

### **4. MLflow Setup**

```bash
# Or use Docker Compose
docker-compose up mlflow
```

### **5. Initial Model Training**

```bash
# Train initial model with MLflow tracking
python train.py

# Register model in MLflow Registry
python scripts/register_model.py
```

### **6. Local Development**

```bash
# Start all services locally
docker-compose up

# Or start individual services
python app.py                    # Flask app on :5001
uvicorn src.fastapi_app.main:app --port 8000  # FastAPI on :8000
streamlit run src/streamlit_app/dashboard.py  # Streamlit on :8501
```

### **7. GitHub Secrets Configuration**

Configure these secrets in your GitHub repository:

```bash
# MLflow credentials
MLFLOW_TRACKING_URI=https://your-mlflow-server.com
MLFLOW_USERNAME=your_username
MLFLOW_PASSWORD=your_password

# AWS credentials for DVC and MLflow artifacts
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key

# Database credentials
POSTGRES_USER=mlflow_user
POSTGRES_PASSWORD=secure_password

# Application secrets
FLASK_SECRET_KEY=your_flask_secret
GRAFANA_PASSWORD=your_grafana_password
```

## ğŸ¯ Key Features

### **Automated Model Retraining**
- **Data drift detection** triggers retraining
- **Performance degradation** monitoring
- **A/B testing** for model comparison
- **Automated rollback** if new model underperforms

### **Comprehensive Monitoring**
- **Real-time predictions** tracking
- **Data quality** monitoring
- **Model performance** drift detection
- **System health** monitoring
- **Business metrics** tracking

### **Multi-Environment Deployment**
- **Development**: Docker Compose locally
- **Staging**: Kubernetes staging cluster
- **Production**: Kubernetes production with scaling
- **Blue-green deployments** for zero downtime

### **Quality Assurance**
- **Automated testing** at multiple levels
- **Code quality** enforcement
- **Security scanning** with Bandit
- **Performance benchmarking**
- **Integration testing** across services

## ğŸš€ Deployment Strategies

### **Local Development**
```bash
# Quick start with Docker Compose
docker-compose up
```

### **Kubernetes Production**
```bash
# Deploy to Kubernetes
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secrets.yaml
kubectl apply -f k8s/
```

## ğŸ“Š Monitoring and Alerting

### **Performance Metrics**
- **Prediction latency** (target: <500ms)
- **Throughput** (requests per second)
- **Error rates** (target: <1%)
- **Model accuracy** (baseline maintenance)

### **Business Metrics**
- **Prediction accuracy** vs ground truth
- **Customer retention** impact
- **False positive/negative** rates
- **Revenue impact** tracking

### **System Health**
- **Service availability** (99.9% uptime)
- **Resource utilization** (CPU, memory)
- **Database performance**
- **Network latency**

## ğŸ” Security Considerations

### **Data Security**
- **Encryption at rest** and in transit
- **Access controls** with RBAC
- **Data anonymization** for non-production
- **Audit logging** for compliance

### **Application Security**
- **Container scanning** for vulnerabilities
- **Dependency scanning** with GitHub Security
- **Secrets management** with Kubernetes secrets
- **Network policies** for isolation

### **Monitoring Security**
- **Authentication** for monitoring dashboards
- **Rate limiting** on API endpoints
- **Input validation** and sanitization
- **Error handling** without information leakage

## ğŸ›ï¸ Operations Guide

### **Daily Operations**
- Monitor dashboard for alerts
- Review prediction accuracy metrics
- Check system health status
- Validate data quality reports

### **Weekly Operations**
- Review model performance trends
- Analyze drift detection reports
- Update training data if needed
- Performance optimization review

### **Monthly Operations**
- Comprehensive model evaluation
- Business impact analysis
- Security patch updates
- Capacity planning review

### **Emergency Procedures**
- **Model rollback** if performance degrades
- **Service scaling** during high load
- **Data drift response** protocols
- **Security incident** response

## ğŸ“ˆ Success Metrics

### **Technical KPIs**
- **Model accuracy**: >82%
- **Prediction latency**: <500ms
- **System uptime**: >99.9%
- **Data drift detection**: <24h response

### **Business KPIs**
- **Customer retention** improvement
- **Prediction precision** for high-risk customers
- **Operational efficiency** gains
- **Cost reduction** through automation

## ğŸ”§ Troubleshooting Guide

### **Common Issues**

#### **Model Loading Failures**
```bash
# Check model file existence
ls -la model/
# Verify DVC pull
dvc status
dvc pull
```

#### **Service Health Issues**
```bash
# Check container logs
docker logs churn_fastapi
kubectl logs -f deployment/fastapi-deployment -n churn-prediction
```

#### **Data Drift Alerts**
```bash
# Run manual drift analysis
python scripts/drift_monitoring.py
# Check data quality
python scripts/data_validation.py
```

#### **Performance Issues**
```bash
# Run performance tests
python scripts/performance_test.py
# Check resource usage
kubectl top pods -n churn-prediction
```

### **Monitoring Commands**

```bash
# Check pipeline status
kubectl get pods -n churn-prediction

# View logs
kubectl logs -f deployment/fastapi-deployment -n churn-prediction

# Check metrics
curl http://localhost:8000/monitoring/performance

# Generate reports
python scripts/generate_monitoring_reports.py
```

## ğŸ¯ Next Steps and Enhancements

### **Phase 2 Enhancements**
- **A/B testing** framework integration
- **Feature stores** (Feast, Tecton)
- **Real-time streaming** with Kafka
- **Advanced explainability** with LIME/SHAP
- **Automated feature engineering**

### **Enterprise Features**
- **Multi-tenant** architecture
- **Federated learning** capabilities
- **Edge deployment** options
- **Advanced security** with OAuth2/OIDC
- **Compliance** reporting (GDPR, SOX)

### **Scaling Considerations**
- **Microservices** architecture
- **Event-driven** processing
- **Caching layers** (Redis)
- **CDN integration**
- **Global deployment** strategies

This comprehensive MLOps pipeline provides enterprise-grade machine learning operations with automated training, monitoring, and deployment capabilities. The modular architecture allows for easy customization and scaling based on your specific requirements.
